{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Traffic Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This solution is about predicting the future behaviour of time series’ that describe the web traffic for Wikipedia articles. The data contains about 145k time series and comes in two separate files: train_2.csv holds the traffic data, where each column is a date and each row is an article, and key_2.csv contains a mapping between page names and a unique ID column (to be used in the submission file).\n",
    "\n",
    "Each of these time series represent a number of daily views of a different Wikipedia article, starting from July, 1st, 2015 up until December 31st, 2016. The leaderboard during the training stage is based on traffic from January, 1st, 2017 up until  September 10th, 2017.\n",
    "\n",
    "This is a competition hosted on Kaggle, the dataset has been taken from Kaggle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Import the Required Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.max_columns = 600\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "from keras.layers import Input, Embedding, Dense, Activation, Dropout, Flatten\n",
    "\n",
    "from keras import regularizers \n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, Dense, Activation, Dropout, Lambda, Multiply, Add, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "import warnings\n",
    "import scipy\n",
    "from datetime import timedelta\n",
    "\n",
    "from pylab import rcParams\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from statsmodels.tsa.tsatools import lagmat\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions are evaluated on Symmetric mean absolute percentage error (SMAPE) between forecasts and actual values.\n",
    "\n",
    "SMAPE is an accuracy measure based on percentage (or relative) errors. It is usually defined as follows:\n",
    "\n",
    "SMAPE = \\frac{1}{n} \\sum\\limits_{t=1}^n{\\frac{|F_t - A_t|}{F_t + A_t}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return np.nanmean(diff)\n",
    "\n",
    "def smape2D(y_true, y_pred):\n",
    "    return smape(np.ravel(y_true), np.ravel(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../data/web-traffic/key_1.csv'),\n",
       " PosixPath('../data/web-traffic/key_2.csv'),\n",
       " PosixPath('../data/web-traffic/sample_submission_1.csv'),\n",
       " PosixPath('../data/web-traffic/sample_submission_2.csv'),\n",
       " PosixPath('../data/web-traffic/train_1.csv'),\n",
       " PosixPath('../data/web-traffic/train_2.csv')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('../data/web-traffic')\n",
    "list(path.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Data files\n",
    "\n",
    "- This challenge is about predicting the future behaviour of time series’ that describe the web traffic for Wikipedia articles. The data contains about 145k time series\n",
    "- The train_1.csv file has data from `2015-07-01` to `2016-12-31`\n",
    "- The train_2.csv file has data from `2015-07-01` to `2017-09-10`\n",
    "- We will be using data from train_2.csv for our training. Which had all data from train_1.csv and additional record. It has records from `July, 1st, 2015` to `September 1st, 2017`\n",
    "- We have to predict daily page views between `September 13th, 2017` to `November 13th, 2017`.\n",
    "- key_*.csv gives the mapping between the page names and the shortened Id column used for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/\"train_1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.read_csv(path/\"train_2.csv\")\n",
    "train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data contains prediction of page views for 803 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's make each page and date into it's individual columns \n",
    "- Also we will check if the given day is a weekend or weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flattened = pd.melt(train_all[list(train_all.columns[-803:])+['Page']], \n",
    "                          id_vars='Page', var_name='date', value_name='Visits')\n",
    "\n",
    "train_flattened['date'] = train_flattened['date'].astype('datetime64[ns]')\n",
    "train_flattened['weekend'] = ((train_flattened.date.dt.dayofweek) // 5 == 1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_median = pd.DataFrame(train_flattened.groupby(['Page'])['Visits'].median())\n",
    "df_median.columns = ['median']\n",
    "\n",
    "df_mean   = pd.DataFrame(train_flattened.groupby(['Page'])['Visits'].mean())\n",
    "df_mean.columns = ['mean']\n",
    "\n",
    "df_std    = pd.DataFrame(train_flattened.groupby(['Page'])['Visits'].std())\n",
    "df_std.columns = ['std']\n",
    "\n",
    "train_flattened = train_flattened.set_index('Page').join(df_mean).join(df_median).join(df_std)\n",
    "train_flattened.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flattened.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation & Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 8))\n",
    "mean_group = train_flattened[['Page', 'date', 'Visits']].groupby(['date'])['Visits'].mean()\n",
    "plt.plot(mean_group)\n",
    "plt.title('Time Series - Average')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 8))\n",
    "median_group = train_flattened[['Page', 'date', 'Visits']].groupby(['date'])['Visits'].median()\n",
    "plt.plot(median_group, color='r')\n",
    "plt.title('Time Series - Average')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 8))\n",
    "std_group = train_flattened[['Page','date','Visits']].groupby(['date'])['Visits'].std()\n",
    "plt.plot(std_group, color = 'g')\n",
    "plt.title('Time Series - std')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check if Language has any Impact on Page View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language(page):\n",
    "    res = re.search('[a-z][a-z].wikipedia.org',page)\n",
    "    if res:\n",
    "        return res[0][0:2]\n",
    "    return 'na'\n",
    "\n",
    "df_train['lang'] = df_train.Page.map(get_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_sets = {}\n",
    "lang_sets['en'] = df_train[df_train.lang=='en'].iloc[:,0:-1]\n",
    "lang_sets['ja'] = df_train[df_train.lang=='ja'].iloc[:,0:-1]\n",
    "lang_sets['de'] = df_train[df_train.lang=='de'].iloc[:,0:-1]\n",
    "lang_sets['na'] = df_train[df_train.lang=='na'].iloc[:,0:-1]\n",
    "lang_sets['fr'] = df_train[df_train.lang=='fr'].iloc[:,0:-1]\n",
    "lang_sets['zh'] = df_train[df_train.lang=='zh'].iloc[:,0:-1]\n",
    "lang_sets['ru'] = df_train[df_train.lang=='ru'].iloc[:,0:-1]\n",
    "lang_sets['es'] = df_train[df_train.lang=='es'].iloc[:,0:-1]\n",
    "\n",
    "sums = {}\n",
    "for key in lang_sets:\n",
    "    sums[key] = lang_sets[key].iloc[:,1:].sum(axis=0) / lang_sets[key].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [r for r in range(sums['en'].shape[0])]\n",
    "\n",
    "fig = plt.figure(1,figsize=[10,10])\n",
    "plt.ylabel('Views per Page')\n",
    "plt.xlabel('Day')\n",
    "plt.title('Pages in Different Languages')\n",
    "labels={'en':'English','ja':'Japanese','de':'German',\n",
    "        'na':'Media','fr':'French','zh':'Chinese',\n",
    "        'ru':'Russian','es':'Spanish'\n",
    "       }\n",
    "\n",
    "for key in sums:\n",
    "    plt.plot(days,sums[key],label = labels[key] )\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['PageTitle'] = df_train.Page.apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_details = pd.DataFrame([i.split(\"_\")[-3:] for i in df_train[\"Page\"]])\n",
    "page_details.columns = [\"site\", \"access\", \"agent\"]\n",
    "page_details.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_columns = page_details['site'].unique()\n",
    "access_columns = page_details['access'].unique()\n",
    "agents_columns = page_details['agent'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_columns = df_train.lang.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_columns = df_train['PageTitle'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(site_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(access_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(agents_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(lang_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(title_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.merge(page_details, how=\"inner\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_by_feature(plot_column, name):\n",
    "    df = df_train.groupby(name).sum().T\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = df.groupby(pd.TimeGrouper('M')).mean().dropna()\n",
    "    df['month'] = 100*df.index.year + df.index.month\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.melt(df, id_vars=['month'], value_vars=plot_column)\n",
    "    fig = plt.figure(1,figsize=[12,10])\n",
    "    ax = sns.pointplot(x=\"month\", y=\"value\", hue=name, data=df)\n",
    "    ax.set(xlabel='Year-Month', ylabel='Mean Hits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_feature(site_columns, \"site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_feature(lang_columns, \"lang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_feature(access_columns, \"access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_feature(agents_columns, \"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_page = train_all.Page.copy()\n",
    "train_key = train_all[['Page']].copy()\n",
    "train_all = train_all.iloc[:,1:] \n",
    "train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_idx = dict((c, i) for i, c in enumerate(train_all.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_idx['2016-09-13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_idx['2017-09-10'] - date_idx['2017-07-09']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '2015-07-01'\n",
    "train_end = '2017-09-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_page_traffic(df, n_series, random_state=42):\n",
    "    \n",
    "    sample = df.sample(n_series, random_state=random_state)\n",
    "    train_series = sample.loc[:,train_start:train_end]\n",
    "    labels = sample['Page'].tolist()\n",
    "    \n",
    "    plt.figure(figsize=(15,12))\n",
    "    \n",
    "    for i in range(train_series.shape[0]):\n",
    "        np.log1p(pd.Series(train_series.iloc[i]).astype(np.float64)).plot(linewidth=1.5)\n",
    "    \n",
    "    plt.title('Page Daily Traffic')\n",
    "    plt.legend(labels)\n",
    "    \n",
    "plot_page_traffic(df_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_page_traffic(df_train, 5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "We need to create 4 sub-segments of the data:\n",
    "\n",
    "1. Train encoding period\n",
    "2. Train decoding period (train targets, 60 days)\n",
    "3. Validation encoding period\n",
    "4. Validation decoding period (validation targets, 60 days)\n",
    "\n",
    "We'll do this by finding the appropriate start and end dates for each segment. Starting from the end of the data we've loaded, we'll work backwards to get validation and training prediction intervals. Then we'll work forward from the start to get training and validation encoding intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_day = pd.to_datetime(train_start) \n",
    "end_day = pd.to_datetime(train_end)\n",
    "\n",
    "steps_size = 60 \n",
    "prediction_length = timedelta(steps_size)\n",
    "\n",
    "\n",
    "\n",
    "val_start_pred = end_day - prediction_length + timedelta(days=1)\n",
    "val_end_pred = end_day\n",
    "\n",
    "train_start_pred = val_start_pred - prediction_length\n",
    "train_end_pred = val_start_pred - timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_length = train_start_pred - start_day\n",
    "\n",
    "train_start_encoded = start_day\n",
    "train_end_encoded = train_start_encoded + encoded_length - timedelta(days=1)\n",
    "\n",
    "val_start_encoded = train_start_encoded + prediction_length\n",
    "val_end_encoded = val_start_encoded + encoded_length - timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(days=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train encoding:', train_start_encoded, '-', train_end_encoded)\n",
    "print('Train prediction:', train_start_pred, '-', train_end_pred, '\\n')\n",
    "print('Val encoding:', val_start_encoded, '-', val_end_encoded)\n",
    "print('Val prediction:', val_start_pred, '-', val_end_pred)\n",
    "\n",
    "print('Encoding interval:', encoded_length.days)\n",
    "print('Prediction interval:', prediction_length.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(days=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['site'] = pd.factorize(train_df.site)[0]\n",
    "train_df['lang'] = pd.factorize(train_df.lang)[0]\n",
    "train_df['PageTitle'] = pd.factorize(train_df.PageTitle)[0]\n",
    "train_df['access'] = pd.factorize(train_df.access)[0]\n",
    "train_df['agent'] = pd.factorize(train_df.agent)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Data Formatting\n",
    "\n",
    "Now that we have the time segment dates, we'll define the functions we need to extract the data in keras friendly format. Here are the steps:\n",
    "\n",
    "Pull the time series into an array, save a date_to_index mapping as a utility for referencing into the array\n",
    "Create function to extract specified time interval from all the series\n",
    "Create functions to transform all the series.\n",
    "Here we smooth out the scale by taking log1p and de-meaning each series using the encoder series mean, then reshape to the (n_series, n_timesteps, n_features) tensor format that keras will expect.\n",
    "Note that if we want to generate true predictions instead of log scale ones, we can easily apply a reverse transformation at prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date_idx = pd.Series(index=pd.Index([pd.to_datetime(c) for c in train_df.columns[1:-5]]),\n",
    "                          data=[i for i in range(len(train_df.columns[1:-5]))])\n",
    "\n",
    "train_series_data = train_df[train_df.columns[1:]].values\n",
    "\n",
    "def time_block_series_data(series_data, date_idx, start_date, end_date):\n",
    "    \n",
    "    indexes = date_idx[start_date:end_date]\n",
    "    return series_data[:,indexes]\n",
    "\n",
    "def encoded_transformed_series(series_data):\n",
    "    \n",
    "    series_data = np.log1p(np.nan_to_num(series_data))\n",
    "    series_mean = series_data.mean(axis=1).reshape(-1,1) \n",
    "    series_data = series_data - series_mean\n",
    "    series_data = series_data.reshape((series_data.shape[0],series_data.shape[1], 1))\n",
    "    \n",
    "    return series_data, series_mean\n",
    "\n",
    "def decoded_transformed_series(series_data, encode_series_mean):\n",
    "    \n",
    "    series_data = np.log1p(np.nan_to_num(series_data))\n",
    "    series_data = series_data - encode_series_mean\n",
    "    series_data = series_data.reshape((series_data.shape[0],series_data.shape[1], 1))\n",
    "    \n",
    "    return series_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 145000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = time_block_series_data(train_series_data, date_idx, \n",
    "                                            train_start_encoded, train_end_encoded)[:samples]\n",
    "encoder_input_data, encode_series_mean = encoded_transformed_series(encoder_input_data)\n",
    "\n",
    "decoder_target_data = time_block_series_data(train_series_data, date_idx, \n",
    "                                            train_start_pred, train_end_pred)[:samples]\n",
    "\n",
    "decoder_target_data = decoded_transformed_series(decoder_target_data, encode_series_mean)\n",
    "\n",
    "lagged_target_history = decoder_target_data[:,:-1,:1]\n",
    "\n",
    "encoder_input_data = np.concatenate([encoder_input_data, lagged_target_history], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building the Model - Architecture\n",
    "\n",
    "This convolutional architecture is a full-fledged version of the WaveNet model, designed as a generative model for audio (in particular, for text-to-speech applications). The wavenet model can be abstracted beyond audio to apply to any time series forecasting problem, providing a nice structure for capturing long-term dependencies without an excessive number of learned weights.\n",
    "\n",
    "The core building block of the wavenet model is the dilated causal convolution layer, discussed in detail in the previous notebook of this series as well as the accompanying blog post. In summary, this style of convolution properly handles temporal flow and allows the receptive field of outputs to increase exponentially as a function of the number of layers. This structure is nicely visualized by the below diagram from the wavenet paper.\n",
    "\n",
    "dilatedconv\n",
    "\n",
    "The model also utilizes some other key techniques: gated activations, residual connections, and skip connections. I'll introduce and explain these techniques, then show how to implement our full-fledged WaveNet architecture in keras. The WaveNet paper diagram below details how the model's components fit together block by block into a stack of operations, so we'll use it as a handy reference as we go (note that there are slight discrepancies between the diagram and what we implement, e.g. the original WaveNet has a softmax classification rather than regression output).\n",
    "\n",
    "\n",
    "**Gated Activations**\n",
    "In the boxed portion of the architecture diagram, you'll notice that the dilated convolution output splits into two branches that are later recombined via element-wise multiplication. This depicts a gated activation unit, where we interpret the tanh activation branch as a learned filter and the sigmoid activation branch as a learned gate that regulates the information flow from the filter. If this reminds you of the gating mechanisms used in LSTMs or GRUs you're on point, as those models use the same style of information gating to control adjustments to their cell states.\n",
    "\n",
    "In mathematical notation, this means we map a convolutional block's input $x$ to output $z$ via the below, where $W_f$ and $W_g$ correspond to (learned) dilated causal convolution weights:\n",
    "\n",
    "$$ z = tanh(W_f * x) \\odot \\sigma(W_g * x) $$\n",
    "Why use gated activations instead of the more standard ReLU activation? The WaveNet designers found that gated activations saw stronger performance empirically than ReLU activations for audio data, and this outperformance may extend broadly to time series data. Perhaps the sparsity induced by ReLU activations is not as well suited to time series forecasting as it is to other problem domains, or gated activations allow for smoother information (gradient) flow over a many-layered WaveNet architecture. However, this choice of activation is certainly not set in stone and I'd be interested to see a results comparison when trying ReLU instead. With that caveat, we'll be sticking with the gated activations in the interest of learning about the full original architecture.\n",
    "\n",
    "**Residual and Skip Connections**\n",
    "In traditional neural network architectures, a neuron layer takes direct input only from the layer that precedes it, so early layers influence deeper layers via a heirarchy of intermediate computations. In theory, this heirarchy allows the network to properly build up high-level predictive features off of lower-level/raw signals. For example, in image classification problems, neural nets start from raw pixel values, find generic geometric and textural patterns, then combine these generic patterns to construct fine-grained representations of the features that identify specific object types.\n",
    "\n",
    "But what if lower-level signals are actually immediately useful for prediction, and may be at risk of distortion as they're passed through a complex heirarchy of computations? We could always simplify the heirarchy by using fewer layers and units, but what if we want the best of both worlds: direct, unfiltered low-level signals and nuanced heirarchical representations? One avenue for addressing this problem is provided by skip connections, which act to preserve earlier feature layer outputs as the network passes forward signals for final prediction processing. To build intuition for why we would want a mix of feature complexities in our problem domain, consider the wide range of time series drivers - there are strong and direct autoregressive components, moderately more sophisticated trend and seasonality components, and idiosyncratic trajectories that are difficult to spot with the human eye.\n",
    "\n",
    "\n",
    "Residual connections are closely related to skip connections; in fact, they can be viewed as specialized, short skips further into the network (often and in our case just one layer). With residual connections, we think of mapping a network block's input to output via $x_{out} = f(x_{in}) + x_{in}$ instead of using the traditional direct mapping $x_{out} = f(x_{in})$, for some function $f$ that corresponds to the model's learned weights. This helps allow for the possibility that the model learns a mapping that acts almost as an identity function, with the input passing through nearly unchanged. In the diagram above, such connections are visualized by the rounded arrows grouped with each pair of convolutions.\n",
    "\n",
    "Why would this be beneficial? Well, the effectiveness of residual connections is still not fully understood, but a compelling explanation is that they facilitate the use of deeper networks by allowing for more direct gradient flow in backpropagation. It's often difficult to efficienctly train the early layers of a deep network due to the length of the backpropagation chain, but residual and skip connections create an easier information highway. Intuitively, perhaps you can think of both as mechanisms for guarding against overcomputation and intermediate signal loss. You can check out the ResNet paper that originated the residual connection concept for more discussion and empirical results.\n",
    "\n",
    "Though our architecture will be shallower than the original WaveNet (fewer convolutional blocks), we'll likely still see some benefit from introducing skip and residual connections at every block. Returning to the WaveNet architecture diagram again, you can see how the residual connection allows each block's input to bypass the convolution stage, and then adds that input to the convolution output. A final point to note is that the diagram's 1x1 convolutions are really just equivalent to (time-distributed) fully connected layers, and serve in post-processing and standardization capacities. Our setup will use layers of this style (with different filter dimensions) for post/pre-processing to facilitate our skip and residual connections, as well as for generating final prediction outputs.\n",
    "\n",
    "**Our Architecture**\n",
    "\n",
    "- 18 dilated causal convolutional blocks\n",
    "- Preprocessing and postprocessing (time distributed) fully connected layers (convolutions with filter width 1): \n",
    "- 18 output units\n",
    "- 32 filters of width 2 per block\n",
    "- Exponentially increasing dilation rate with a reset (1, 2, 4, 8, 16, 32, 64, 128, 256, 1, 2, 4, 8, 16, 32, 64, 128, 256)\n",
    "- Gated activations \n",
    "- Residual and skip connections\n",
    "- 1 (time distributed) fully connected layers to map sum of skip outputs to final output\n",
    "\n",
    "We'll extract the last 60 steps from the output sequence as our predicted output for training. We'll use teacher forcing again during training. Similarly to the previous notebook, we'll have a separate function that runs an inference loop to generate predictions on unseen data, iteratively filling previous predictions into the history sequence (section 4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(dilation_rates):\n",
    "    inp = Input(shape=(None, 1))\n",
    "    x = inp\n",
    "    skips = []\n",
    "\n",
    "    for dilation_rate in dilation_rates:\n",
    "        x = Conv1D(filters=16,\n",
    "                   kernel_size=1,\n",
    "                   padding='same',\n",
    "                   activation='relu')(x) \n",
    "\n",
    "        x_f = Conv1D(filters=32,\n",
    "                     kernel_size=2, \n",
    "                     padding='causal',\n",
    "                     dilation_rate=dilation_rate)(x)\n",
    "\n",
    "        x_g = Conv1D(filters=32,\n",
    "                     kernel_size=2, \n",
    "                     padding='causal',\n",
    "                     dilation_rate=dilation_rate)(x)\n",
    "\n",
    "        z = Multiply()([Activation('tanh')(x_f),\n",
    "                        Activation('sigmoid')(x_g)])\n",
    "\n",
    "        z = Conv1D(filters=16, \n",
    "                   kernel_size=1, \n",
    "                   padding='same', \n",
    "                   activation='relu')(z)\n",
    "\n",
    "        x = Add()([x, z])    \n",
    "\n",
    "        skips.append(z)\n",
    "            \n",
    "    out = Activation('relu')(Add()(skips))\n",
    "    \n",
    "    out = Conv1D(filters=256, kernel_size=1, padding='same')(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Dropout(rate=.5)(out)\n",
    "    \n",
    "    out = Conv1D(filters=128, kernel_size=1, padding='same')(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Dropout(rate=.2)(out)\n",
    "    \n",
    "    out = Conv1D(filters=1, kernel_size=1, padding='same')(out)\n",
    "    \n",
    "    train_pred = Lambda(lambda x, seq: x[:,-seq:,:], \n",
    "                            arguments={'seq':60})(out)\n",
    "    \n",
    "    model = Model(inp, train_pred)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilation_rates = [1, 2, 4, 8, 16, 32, 64, 128, 256, 1, 2, 4, 8, 16, 32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([1, 2, 4, 8, 16, 32, 64, 128, 256, 1, 2, 4, 8, 16, 32, 64, 128, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(dilation_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=1e-2, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "model.compile(optimizer=optimizer, loss='mean_absolute_error')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='web_traffic_best.hdf5',\n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "              min_delta=0,\n",
    "              patience=5,\n",
    "              verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(encoder_input_data, decoder_target_data,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=[checkpointer, learning_rate_reduction, early_stopping],\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error Loss')\n",
    "plt.title('Loss Over Time')\n",
    "plt.legend(['Train','Valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Like in the previous notebook, we'll generate predictions by running our model from section 3 in a loop, using each iteration to extract the prediction for the time step one beyond our current history then append it to our history sequence. With 60 iterations, this lets us generate predictions for the full interval we've chosen.\n",
    "\n",
    "Recall that we designed our model to output predictions for 60 time steps at once in order to use teacher forcing for training. So if we start from a history sequence and want to predict the first future time step, we can run the model on the history sequence and take the last time step of the output, which corresponds to one time step beyond the history sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs):\n",
    "    inp = inp.copy()\n",
    "    preds = np.zeros((1, steps_size, 1))\n",
    "    \n",
    "    for i in range(steps_size):\n",
    "        last_pred = model.predict(history_sequence)[0,-1,0]\n",
    "        preds[0,i,0] = last_pred\n",
    "        \n",
    "        # add the next time step prediction to the history sequence\n",
    "        inp = np.concatenate([inp, last_pred.reshape(-1,1,1)], axis=1)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generating and Plotting Predictions\n",
    "\n",
    "Now we have everything we need to generate predictions for encoder (history) /target series pairs that we didn't train on (note again we're using \"encoder\"/\"decoder\" terminology to stay consistent with notebook 1 -- here it's more like history/target). We'll pull out our set of validation encoder/target series (recall that these are shifted forward in time). Then using a plotting utility function, we can look at the tail end of the encoder series, the true target series, and the predicted target series. This gives us a feel for how our predictions are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = time_block_series_data(train_series_data, date_idx, \n",
    "                                            val_start_encoded, val_end_encoded)[:samples]\n",
    "encoder_input_data, encode_series_mean = encoded_transformed_series(encoder_input_data)\n",
    "\n",
    "decoder_target_data = time_block_series_data(train_series_data, date_idx, \n",
    "                                            val_start_pred, val_end_pred)[:samples]\n",
    "\n",
    "decoder_target_data = decoded_transformed_series(decoder_target_data, encode_series_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(encoder_input_data, decoder_target_data, sample_index, enc_tail_len=50):\n",
    "\n",
    "    encode_series = encoder_input_data[sample_index:sample_index+1,:,:] \n",
    "    pred_series = predict_sequence(encode_series)\n",
    "    \n",
    "    encode_series = encode_series.reshape(-1,1)\n",
    "    pred_series = pred_series.reshape(-1,1)   \n",
    "    target_series = decoder_target_data[sample_index,:,:1].reshape(-1,1) \n",
    "    \n",
    "    encode_series_tail = np.concatenate([encode_series[-enc_tail_len:],target_series[:1]])\n",
    "    x_encode = encode_series_tail.shape[0]\n",
    "    \n",
    "    plt.figure(figsize=(10,6))   \n",
    "    \n",
    "    plt.plot(range(1,x_encode+1),encode_series_tail)\n",
    "    plt.plot(range(x_encode,x_encode+pred_steps),target_series,color='orange')\n",
    "    plt.plot(range(x_encode,x_encode+pred_steps),pred_series,color='teal',linestyle='--')\n",
    "    \n",
    "    plt.title('Encoder Series Tail of Length %d, Target Series, and Predictions' % enc_tail_len)\n",
    "    plt.legend(['Encoding Series','Target Series','Predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating some plots as below, we can see that our longer time horizon predictions (60 days) are often strong and expressive. Our full-fledged model is able to effectively capture weekly seasonality patterns and long term trends, and does a very nice job adapting to the varying levels of fluctuation in each series.\n",
    "\n",
    "Still, we can do even better! We'd benefit from increasing the sample size for training and fine-tuning our hyperparameters, but also by giving the model access to additional relevant information. So far we've only fed the model raw time series data, but it can likely benefit from the inclusion of exogenous variables such as the day of the week and the language of the webpage corresponding to each series. To see how these exogenous variables can be incorporated directly into the model check out the next notebook in this series.\n",
    "\n",
    "If you're interested in digging even deeper into state of the art WaveNet style architectures, I also highly recommend checking out Sean Vasquez's model that was designed for this data set. He implements a customized seq2seq WaveNet architecture in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_plot(encoder_input_data, decoder_target_data, \n",
    "                 sample_ind=16534, enc_tail_len=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futher Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
