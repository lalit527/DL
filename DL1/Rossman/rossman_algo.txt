Structured learning
1. Load Libraries Structured, column_data, Ipython.display
2. Path of the data file.
3. Read all these tables, ['train', 'store', 'store_states', 'state_names', 'googletrend', 'weather', 'test']
4. Loop over tales and show all the tables. display(t.head)
5. destructure tables into their respective table names.['train', 'store', 'store_states', 'state_names', 'googletrend', 'weather', 'test']
6. Check length of train and test.
7. Set train and test StateHoldays to 0.
8. Create a Method Join Df.left, right, left_on, right_on=None, suffix='_y'
   right_on==None, right_on=left_on
   left.merge(right, how'left')
9. weather join with state_names over 'file', 'statename' and store in weather
10. In Googletrend create Date from week.str.split, State from file.str.split
    Replace all NI statename with 'HB,NI'
11. add datepart to weather-'Date', googletrend-'Date', train-'Date', test-Date 
12. Get trend_de = googletrend[googletrend.file == 'Rossmann_DE']
13. Join Store and store_states over 'state' and store in store.
14. Join train and store over "Store" and store in joined, joined_test for test and store.
15. join joined with googletrend, joined_test with googletrend over ["State","Year", "Week"]
16. joined and joined_test merge trend_de params as 'left', ["Year", "Week"], suffixes=('', '_DE')
17. joined and joined_test join over weather, '["State","Date"]'
18. For joined and joined_test drop columns that ends with '_y'
19. For joined and joined_test fill na columns.
    CompetitionOpenSinceYear -> fillna(1900).astype(np.int32)
    CompetitionOpenSinceMonth -> fillna(1).astype(np.int32)
    Promo2SinceYear -> fillna(1900).astype(np.int32)
    Promo2SinceWeek -> fillna(1).astype(np.int32)
20. For joined and joined_test create new columns.
    df["CompetitionOpenSince"] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear,  month=df.CompetitionOpenSinceMonth, day=15))
    df["CompetitionDaysOpen"] = df.Date.subtract(df.CompetitionOpenSince).dt.days
21. For joined and joined_test replace some erroneous / outlying data.
    df.loc[df.CompetitionDaysOpen<0, "CompetitionDaysOpen"] = 0
    df.loc[df.CompetitionOpenSinceYear<1990, "CompetitionDaysOpen"] = 0
22. For joined and joined_test replace some erroneous / outlying data.
    df.loc[df.CompetitionDaysOpen<0, "CompetitionDaysOpen"] = 0
    df.loc[df.CompetitionOpenSinceYear<1990, "CompetitionDaysOpen"] = 0
23. Promo Dates:-
    df["Promo2Since"] = pd.to_datetime(df.apply(lambda x: Week(x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1).astype(pd.datetime))
    df["Promo2Days"] = df.Date.subtract(df["Promo2Since"]).dt.days
24. df.loc[df.Promo2Days<0, "Promo2Days"] = 0
    df.loc[df.Promo2SinceYear<1990, "Promo2Days"] = 0
    df["Promo2Weeks"] = df["Promo2Days"]//7
    df.loc[df.Promo2Weeks<0, "Promo2Weeks"] = 0
    df.loc[df.Promo2Weeks>25, "Promo2Weeks"] = 25
    df.Promo2Weeks.unique()
25. Save joined and joined_test to_feather
26. Definition get_elapsed(fld, pre)
    '
    day1 = np.timedelta64(1, 'D')
    last_date = np.datetime64()
    last_store = 0
    res = []
    for s,v,d in zip(df.Store.values,df[fld].values, df.Date.values):
      if s != last_store:
            last_date = np.datetime64()
            last_store = s
        if v: last_date = d
        res.append(((d-last_date).astype('timedelta64[D]') / day1))
    df[pre+fld] = res
    '
27. columns = ["Date", "Store", "Promo", "StateHoliday", "SchoolHoliday"]
28. df = train[columns].append(test[columns])
29. fld = 'SchoolHoliday'
    df = df.sort_values(['Store', 'Date'])
    get_elapsed(fld, 'After')
    df = df.sort_values(['Store', 'Date'], ascending=[True, False])
    get_elapsed(fld, 'Before')
30. Perform same for 'StateHoliday' & 'Promo'
31. Set Date as Index on df.
32. set null values from elapsed field calculations to 0.
    columns = ['SchoolHoliday', 'StateHoliday', 'Promo']
    '
    for o in ['Before', 'After']:
    for p in columns:
        a = o+p
        df[a] = df[a].fillna(0).astype(int)
    '
33. Sorting by date (sort_index()) and counting the number of events of interest (sum()) 
    defined in columns in the following week (rolling()), grouped by Store (groupby()).
    bwd = df[['Store']+columns].sort_index().groupby("Store").rolling(7, min_periods=1).sum()
    fwd = df[['Store']+columns].sort_index(ascending=False
                                      ).groupby("Store").rolling(7, min_periods=1).sum()
34. drop the Store indices. Reset Index on df, bwd, fwd.
35. Df Merge with bwd, fwd - left, ['Date', 'Store'], suffix bw, fw.
36. Drop Columns inplace.
37. date on df to pd.to_datetime(df.date)
38. joined = join_df(joined, df, ['Store', 'Date']), joined_test.
39. joined = joined[joined.Sales!=0]
40. reset_index on joined and joined_test.
41. joined and joined_test to_feather.
42. Seperate categorical and continuous variable.
    cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',
    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',
    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',
    'SchoolHoliday_fw', 'SchoolHoliday_bw']

    contin_vars = ['CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',
      'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', 
      'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',
      'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday']

    n = len(joined); n
43. dep = 'Sales'
    joined = joined[cat_vars+contin_vars+[dep, 'Date']].copy()

    joined_test[dep] = 0
    joined_test = joined_test[cat_vars+contin_vars+[dep, 'Date', 'Id']].copy()
44. Change to categorical variable.
    for v in cat_vars: joined[v] = joined[v].astype('category').cat.as_ordered()
45. apply_cat on joined_test from joined.
46. contin_vars -- joined and joined_test fillna(0).astype(float32)
47. get_cv_idxs, joined.iloc, samp_size = len(joined_samp)
48. samp_size = n
    joined_samp = joined.set_index("Date")
49. 
    '
    df, y, nas, mapper = proc_df(joined_samp, 'Sales', do_scale=True)
    yl = np.log(y)

    joined_test = joined_test.set_index("Date")

    df_test, _, nas, mapper = proc_df(joined_test, 'Sales', do_scale=True, skip_flds=['Id'],
                                  mapper=mapper, na_dict=nas)
    '
50. 
    train_ratio = 0.75
    train_size = int(samp_size * train_ratio); train_size
    val_idx = list(range(train_size, len(df)))
51. val_idx = np.flatnonzero((df.index<=datetime.datetime(2014,9,17)) & (df.index>=datetime.datetime(2014,8,1)))
52. val_idx=[0]

DL
53. inv_y :- np.exp()
    exp_rmspe
    max_log_y =  np.max(yl)
    y_range = (0, max_log_y*1.2)
54. ColumnarData.from data frame create learner.
55. Create cat_sz count of element in categorical data.
    [('Store', 1116),
    ('DayOfWeek', 8), ...]
56. create embedding sz size of cat_sz and min(50, len(cat_sz) // 2).
    [(1116, 50),
      (8, 4), ...]
57. md.get_learner - emb_szs, len(df.columns)-len(cat_vars),  0.04, 1, [1000,500], [0.001,0.01], y_range=y_range
58. lr=1e-3, lr_find()
59. m.fit(lr, 1, metrics=[exp_rmspe]), m.fit(lr, 3, metrics=[exp_rmspe]), m.fit(lr, 3, metrics=[exp_rmspe], cycle_len=1)
60. x,y=m.predict_with_targs(), exp_rmspe(x,y)