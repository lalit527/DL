{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://files.grouplens.org/datasets/movielens/ml-latest-small.zip --output ml-latest-small.zip \n",
    "!mkdir -p ml-latest-small\n",
    "!unzip ml-latest-small.zip -d sample_data/ml-latest-small\n",
    "!pip install fastai==0.7.0\n",
    "import sys\n",
    "!{sys.executable} -m pip install torchtext==0.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "- using MovieLens dataset Movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.learner import *\n",
    "from fastai.column_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'sample_data/ml-latest-small/ml-latest-small/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're working with the movielens data, which contains one rating per row, like below.  \n",
    "We will use `userId` (categorical), `movieId`(categorical) and `rating` (dependent) for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(PATH+'ratings.csv')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for display purposes, let's read in the movie names too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(PATH+'movies.csv')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create subset for Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a crosstab of the most popular movies and most movie-addicted users which we'll copy into Excel for creating a simple example. This isn't necessary for any of the modeling below however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ratings.groupby('userId')['rating'].count()\n",
    "topUsers = g.sort_values(ascending=False)[:15]\n",
    "\n",
    "g = ratings.groupby('movieId')['rating'].count()\n",
    "topMovies = g.sort_values(ascending=False)[:15]\n",
    "\n",
    "top_r = ratings.join(topUsers, rsuffix='_r', how='inner', on='userId')\n",
    "top_r = top_r.join(topMovies, rsuffix='_r', how='inner', on='movieId')\n",
    "\n",
    "pd.crosstab(top_r.userId, top_r.movieId, top_r.rating, aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we will use matrix factorization/decomposition instead of building a neural net.\n",
    "\n",
    "Each prediction is a dot product of movie embedding vector and user embedding vector. In linear algebra term, it is equivalent of matrix product as one is a row and one is a column. If there is no actual rating, we set the prediction to zero (think of this as test data — not training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a validation set by picking random set of ID’s. wd is a weight decay for L2 regularization, and n_factors is how big an embedding matrix we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idxs = get_cv_idxs(len(ratings))\n",
    "wd = 2e-4\n",
    "n_factors = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a model data object from CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = CollabFilterDataset.from_csv(PATH, 'ratings.csv', 'userId', 'movieId', 'rating')\n",
    "learn = cf.get_learner(n_factors, val_idxs, 64, opt_fn=optim.Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then get a learner that is suitable for the model data, and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1e-2, 2, wds=wd, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output is Mean Squared Error, you can take RMSE by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.sqrt(0.765)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how the predictions compare to actuals for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = learn.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can also plot using seaborn sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = learn.data.val_y\n",
    "sns.jointplot(preds, y, kind='hex', stat_func=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_names = movies.set_index('movieId')['title'].to_dict()\n",
    "g = ratings.groupby('movieId')['rating'].count()\n",
    "topMovies = g.sort_values(ascending=False).index.values[:3000]\n",
    "topMoviesIdx = np.array([cf.item2idx[o] for o in topMovies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = learn.model; m.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll look at the movie bias term. Here, our input is the movie id (a single id), and the output is the movie bias (a single float)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.ib(V(topMoviesIdx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_bias = to_np(m.ib(V(topMoviesIdx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings = [(b[0], movie_names[i]) for i, b in zip(topMovies, movie_bias)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(movie_ratings, key=lambda o: o[0])[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(movie_ratings, key=itemgetter(0))[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(movie_ratings, key=lambda o: o[0], reverse=True)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do the same thing for the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_emb = to_np(m.i(V(topMoviesIdx)))\n",
    "movie_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it's hard to interpret 50 embeddings, we use PCA to simplify them down to just 3 vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "movie_pca = pca.fit(movie_emb.T).components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fac0 = movie_pca[0]\n",
    "movie_comp = [(f, movie_names[i]) for f, i in zip(fac0, topMovies)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the 1st component. It seems to be 'easy watching' vs 'serious'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(movie_comp, key=itemgetter(0), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(movie_comp, key=itemgetter(0))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fac1 = movie_pca[1]\n",
    "movie_comp = [(f, movie_names[i]) for f, i in zip(fac1, topMovies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(movie_comp, key=itemgetter(0), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(movie_comp, key=itemgetter(0))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.random.choice(len(topMovies), 50, replace=False)\n",
    "X = fac0[idxs]\n",
    "Y = fac1[idxs]\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.scatter(X, Y)\n",
    "for i, x, y in zip(topMovies[idxs], X, Y):\n",
    "    plt.text(x, y, movie_names[i], color=np.random.rand(3)*0.7, fontsize=11)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collab filtering from scratch¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot product example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = T([[1., 2], [3, 4]])\n",
    "b = T([[2., 2], [10, 10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have a mathematical operator between tensors in numpy or PyTorch, it will do element-wise assuming that they both have the same dimensionality. The below is how you would calculate the dot product of two vectors (e.g. (1, 2)⋅(2, 2) = 6 — the first rows of matrix a and b):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 70.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a*b).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProduct(nn.Module):\n",
    "    def forward(self, u, m):\n",
    "        return (u*m).sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call it and get the expected result (notice that we do not need to say model.forward(a, b) to call the forward function — it is a PyTorch magic.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DotProduct()\n",
    "model(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building more complex module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation has two additions to the DotProduct class:\n",
    "\n",
    "- Two nn.Embedding matrices\n",
    "- Look up our users and movies in above embedding matrices\n",
    "\n",
    "It is quite possible that user ID’s are not contiguous which makes it hard to use as an index of embedding matrix. So we will start by creating indexes that starts from zero and contiguous and replace ratings.userId column with the index by using Panda’s apply function with an anonymous function lambda and do the same for ratings.movieId "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot product model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_uniq = ratings.userId.unique()\n",
    "user2idx = {o: i for i, o in enumerate(m_uniq)}\n",
    "ratings.userId = ratings.userId.apply(lambda x: user2idx[x])\n",
    "\n",
    "m_uniq = ratings.movieId.unique()\n",
    "movie2idx = {o: i for i, o in enumerate(m_uniq)}\n",
    "ratings.movieId = ratings.movieId.apply(lambda x: movie2idx[x])\n",
    "\n",
    "n_users = int(ratings.userId.nunique())\n",
    "n_movies = int(ratings.movieId.nuinque())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot(nn.Module):\n",
    "    def __init__(self, n_users, n_movies):\n",
    "        super().__init__()\n",
    "        self.u = nn.Embedding(n_users, n_factors)\n",
    "        self.m = nn.Embedding(n_movies, n_factors)\n",
    "        self.u.weight.data.uniform_(0, 0.05)\n",
    "        self.m.weight.data.uniform_(0, 0.05)\n",
    "        \n",
    "    def forward(self, cats, conts):\n",
    "        users, movies = cats[:, 0], cats[:, 1]\n",
    "        u, m = self.u(users), self.m(movies)\n",
    "        return (u*m).sum(1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding is not a tensor but a variable. A variable does the exact same operations as a tensor but it also does automatic differentiation. To pull a tensor out of a variable, call data attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ratings.drop(['rating', 'timestamp'], axis=1)\n",
    "y = ratings['rating'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ColumnarModelData.from_data_frame(PATH, val_idxs, x, y, ['userId', 'movieId'], 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are reusing ColumnarModelData (from fast.ai library) from Rossmann notebook, and that is the reason behind why there are both categorical and continuous variables in def forward(self, cats, conts) function in EmbeddingDot\n",
    "\n",
    "Since we do not have continuous variable in this case, we will ignore conts and use the first and second columns of cats as users and movies . Note that they are mini-batches of users and movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 1e-5\n",
    "model = EmbeddingDot(n_users, n_movies).cuda()\n",
    "opt = optim.SGD(model.parameters(), 1e-1, weight_decay=wd, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optim is what gives us the optimizers in PyTorch. model.parameters() is one of the function inherited from nn.Modules that gives us all the weight to be updated/learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, data, 3, opt, F.mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, data, 3, opt, F.mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rating, max_rating = ratings.rating.min(), ratings.rating.max()\n",
    "min_rating, max_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let’s improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bias** — to adjust to generally popular movies or generally enthusiastic users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(ni, nf):\n",
    "    e = nn.Embedding(ni, nf)\n",
    "    e.weight.data.uniform_(-0.01, 0.01)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDotBias(nn.Module):\n",
    "    def __init__(self, n_users, n_movies):\n",
    "        super().__init__()\n",
    "        (self.u, self.m, self.ub, self.mb) = [get_emb(*o) for o in [\n",
    "            (n_users, n_factors), (n_movies, n_factors), (n_users, 1), (n_movies, 1)\n",
    "        ]]\n",
    "        \n",
    "    def forward(self, cats, conts):\n",
    "        users, movies = cats[:, 0], cats[:, 1]\n",
    "        um = (self.u(users) * self.m(movies)).sum(1)\n",
    "        res = um + self.ub(users).squeeze() + self.mb(movies).squeeze()\n",
    "        res = F.sigmoid(res) * (max_rating - min_rating) + min_rating\n",
    "        return res.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=2e-4\n",
    "model = EmbeddingDotBias(cf.n_users, cf.n_items).cuda()\n",
    "opt = optim.SGD(model.parameters(), 1e-1, weight_decay=wd, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, data, 3, opt, F.mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, data, 3, opt, F.mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than calculating the dot product of user embedding vector and movie embedding vector to get a prediction, we will concatenate the two and feed it through neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, nh=10, p1=0.05, p2=0.5):\n",
    "        super().__init__()\n",
    "        self.u, self.m) = [get_emb(*o) for o in [\n",
    "            (n_users, n_factors), (n_movies, n_factors)]]\n",
    "        self.lin1 = nn.Linear(n_factors*2, nh)\n",
    "        self.lin2 = nn.Linear(nh, 1)\n",
    "        self.drop1 = nn.Dropout(p1)\n",
    "        self.drop2 = nn.Dropout(p2)\n",
    "        \n",
    "    def forward(self, cats, conts):\n",
    "        users, movies = cats[:, 0], cats[:, 1]\n",
    "        x = self.drop1(torch.cat([self.u(users), self.m(movies)], dim=1))\n",
    "        x = self.drop2(F.relu(self.lin1(x)))\n",
    "        return F.sigmoid(self.lin2(x)) * (max_rating - min_rating + 1) + min_rating - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we no longer have bias terms since Linear layer in PyTorch already has a build in bias. nh is a number of activations a linear layer creates (Jeremy calls it “num hidden”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only has one hidden layer, so maybe not “deep”, but this is definitely a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-5\n",
    "model = EmbeddingNet(n_users, n_movies).cuda()\n",
    "opt = optim.Adam(model.parameters(), 1e-3, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, data, 3, opt, F.mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, data, 3, opt, F.mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the loss functions are also in F (here, it s mean squared loss).\n",
    "\n",
    "Now that we have neural net, there are many things we can try:\n",
    "\n",
    "- Add dropouts\n",
    "- Use different embedding sizes for user embedding and movie embedding\n",
    "- Not only user and movie embeddings, but append movie genre embedding and/or timestamp from the original data.\n",
    "- Increase/decrease number of hidden layers and activations\n",
    "- Increase/decrease regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we are passing off the updating of weights to PyTorch’s optimizer. What does an optimizer do? and what is a momentum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
