{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_char_rnn.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxUUxtsH_qQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from theano import sandbox"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrgVsLlOArO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLoQoqYnBJvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.utils import np_utils\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\n",
        "from keras.layers import TimeDistributed, Activation, SimpleRNN, GRU\n",
        "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import SGD, RMSprop, Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyGPAL6lBNdl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46d002ef-e18a-4399-8fdf-eba1f2f467e2"
      },
      "source": [
        "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
        "text = open(path).read().lower()\n",
        "print('corpus length:', len(text))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus length: 600893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhhg3re6CI6m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "fb819e76-0496-4bfb-e692-cbca243a4539"
      },
      "source": [
        "!tail {path} -n25"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "are thinkers who believe in the saints.\n",
            "\n",
            "\n",
            "144\n",
            "\n",
            "It stands to reason that this sketch of the saint, made upon the model\n",
            "of the whole species, can be confronted with many opposing sketches that\n",
            "would create a more agreeable impression. There are certain exceptions\n",
            "among the species who distinguish themselves either by especial\n",
            "gentleness or especial humanity, and perhaps by the strength of their\n",
            "own personality. Others are in the highest degree fascinating because\n",
            "certain of their delusions shed a particular glow over their whole\n",
            "being, as is the case with the founder of christianity who took himself\n",
            "for the only begotten son of God and hence felt himself sinless; so that\n",
            "through his imagination--that should not be too harshly judged since the\n",
            "whole of antiquity swarmed with sons of god--he attained the same goal,\n",
            "the sense of complete sinlessness, complete irresponsibility, that can\n",
            "now be attained by every individual through science.--In the same manner\n",
            "I have viewed the saints of India who occupy an intermediate station\n",
            "between the christian saints and the Greek philosophers and hence are\n",
            "not to be regarded as a pure type. Knowledge and science--as far as they\n",
            "existed--and superiority to the rest of mankind by logical discipline\n",
            "and training of the intellectual powers were insisted upon by the\n",
            "Buddhists as essential to sanctity, just as they were denounced by the\n",
            "christian world as the indications of sinfulness."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxBCdRQpCM_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08d98a29-b99d-4c27-b2ad-73886f857722"
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars) + 1\n",
        "print('Total Chracters:', vocab_size)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Chracters: 58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCdfB6gsCgXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars.insert(0, '\\0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mxa-CdEClkX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b9a76ea-cf0b-4703-814c-e82ff58a1fb2"
      },
      "source": [
        "''.join(chars)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\x00\\n !\"\\'(),-.0123456789:;=?[]_abcdefghijklmnopqrstuvwxyzäæéë'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LP2g8DMCoL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1xT9xGTC6YH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = [char_indices[c] for c in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nkfj0c0ZC9v3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d565d67-c0e6-43a1-f3ec-e6d9c4a6b596"
      },
      "source": [
        "idx[:10]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[43, 45, 32, 33, 28, 30, 32, 1, 1, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOnCJFNlC_V_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76394e0a-aa98-48be-e847-118a355fa5a3"
      },
      "source": [
        "''.join(indices_char[i] for i in idx[:70])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'preface\\n\\n\\nsupposing that truth is a woman--what then? is there not gro'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCy_6P0zDCq6",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess and create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TYyWXHLDBZX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5bd8e9b-0d99-43fe-d0b0-8672ac7bd5b2"
      },
      "source": [
        "maxlen = 40\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(idx) - maxlen + 1):\n",
        "    sentences.append(idx[i: i + maxlen])\n",
        "    next_chars.append(idx[i+1: i+maxlen+1])\n",
        "print('nb_sequences:', len(sentences))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb_sequences: 600854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hqzdIcZE1x0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = np.concatenate([[np.array(o)] for o in sentences[:-2]])\n",
        "next_chars = np.concatenate([[np.array(o)] for o in next_chars[:-2]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uKz7OGcFEFV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3870be65-9bc1-47f4-8c58-35fbe73fe13d"
      },
      "source": [
        "sentences.shape, next_chars.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((600852, 40), (600852, 40))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8cc40Z2FHZy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "0ffd9bf1-e94f-40ca-f244-537c5df1f16b"
      },
      "source": [
        "sentences[0]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([43, 45, 32, 33, 28, 30, 32,  1,  1,  1, 46, 48, 43, 43, 42, 46, 36,\n",
              "       41, 34,  2, 47, 35, 28, 47,  2, 47, 45, 48, 47, 35,  2, 36, 46,  2,\n",
              "       28,  2, 50, 42, 40, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lRiNMAkFqPQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7936c859-4e46-4be0-bc89-602dc534165e"
      },
      "source": [
        "next_chars[0]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([45, 32, 33, 28, 30, 32,  1,  1,  1, 46, 48, 43, 43, 42, 46, 36, 41,\n",
              "       34,  2, 47, 35, 28, 47,  2, 47, 45, 48, 47, 35,  2, 36, 46,  2, 28,\n",
              "        2, 50, 42, 40, 28, 41])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weOba7SRFYyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79b973b9-50ec-441b-8d7a-4359ee75b99b"
      },
      "source": [
        "''.join(indices_char[i] for i in sentences[0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'preface\\n\\n\\nsupposing that truth is a woma'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iGrc0Q_FkYI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17b604d2-8004-4973-b42c-43698a7b4ca6"
      },
      "source": [
        "''.join(indices_char[i] for i in next_chars[0])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'reface\\n\\n\\nsupposing that truth is a woman'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3YYbFUCFn8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fac = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtKcJ2pKFu-K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "ef2669a9-1229-4f0d-e1f6-6c1015c2b896"
      },
      "source": [
        "model = Sequential([\n",
        "    Embedding(vocab_size, n_fac, input_length=maxlen),\n",
        "    LSTM(512, input_dim=n_fac, return_sequences=True, dropout_U=0.2, dropout_W=0.2, consume_less='gpu'),\n",
        "    Dropout(0.2),\n",
        "    LSTM(512, return_sequences=True, dropout_U=0.2, dropout_W=0.2, consume_less='gpu'),\n",
        "    Dropout(0.2),\n",
        "    TimeDistributed(Dense(vocab_size)),\n",
        "    Activation('softmax')\n",
        "])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, return_sequences=True, input_shape=(None, 42), dropout=0.2, recurrent_dropout=0.2, implementation=2)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, return_sequences=True, dropout=0.2, recurrent_dropout=0.2, implementation=2)`\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIa8CtW3GQ-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wolmrI78GaWf",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RB9_CCfGZXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_example():\n",
        "    seed_string = 'ethics is a basic foundation of all that'\n",
        "    for i in range(320):\n",
        "        x=np.array([char_indices[c] for c in seed_string[-40:]])[np.newaxis,:]\n",
        "        preds = model.predict(x, verbose=0)[0][-1]\n",
        "        preds = preds/np.sum(preds)\n",
        "        next_char = choice(chars, p=preds)\n",
        "        seed_string = seed_string + next_char\n",
        "    print(seed_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6XhGPLrG_ov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "25444dc5-6b18-42c9-ebc2-4109c147b085"
      },
      "source": [
        "model.fit(sentences, np.expand_dims(next_chars,-1), batch_size=64, nb_epoch=1)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "600852/600852 [==============================] - 1025s 2ms/step - loss: 1.4815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbdf07a2da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1_51pyQHCR_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "277f8140-5c79-4c65-edda-ddad4c387947"
      },
      "source": [
        "print_example()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ethics is a basic foundation of all that will which considered there is\n",
            "not mistaken. it is no way of \"-natural\"--guld fting that is dangerous and changed that we are\n",
            "contradictively finally speak of women, who,\n",
            "has not hitherto no existence and politics, in control of the new, no\n",
            "organic and more than this other philosophy, but not--what does plainly be\n",
            "gui\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BUenx-iHWp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.optimizer.lr=0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a32q3oADLMQK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "afe2d887-23eb-4a35-f46e-747089f490e7"
      },
      "source": [
        "model.fit(sentences, np.expand_dims(next_chars,-1), batch_size=64, nb_epoch=1)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "\r    64/600852 [..............................] - ETA: 23:42 - loss: 1.2611"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "600852/600852 [==============================] - 1028s 2ms/step - loss: 1.2791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbd92663c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1_ZhsoMLOWK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "0eefd3f2-b271-42f7-b270-737c231b4231"
      },
      "source": [
        "print_example()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ethics is a basic foundation of all that\n",
            "is \"each other.\" the\n",
            "ascetic soul was proceeding\n",
            "to it: from\n",
            "the kind of\n",
            "invention,\n",
            "     which books upon further, evilscent rank, just the mother and consequences of\n",
            "good and way through the seems to\n",
            "understand himself--or -lett, indeed, industry, the old philosopher with other spirits; they _appear i movement that f\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBWJxnqrPgag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model.optimizer.lr=1e-2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J3mKkegPz1K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "3f78f07f-fe50-45ba-cb87-4d0dc0ae337a"
      },
      "source": [
        "model.fit(sentences, np.expand_dims(next_chars,-1), batch_size=64, nb_epoch=1)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "\r    64/600852 [..............................] - ETA: 24:31 - loss: 1.2496"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "600852/600852 [==============================] - 1016s 2ms/step - loss: 1.2467\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbdd9bce9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR0qFGXZQKpq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "5f467af7-4aab-4d4c-87a2-d0392865e1aa"
      },
      "source": [
        "print_example()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ethics is a basic foundation of all that the realm of that\n",
            "things from a bell, to assust\n",
            "to him.\n",
            "\n",
            "231. when people again our\n",
            "problem of rank; for the classes of their very requirements than any\n",
            "kind of his own\n",
            "teacher, \"matter\" about\n",
            "it by the science\n",
            "indeed all the meanwhile retrouded the\n",
            "soul, the riddle of the \"soul,\" had to give\n",
            "ourselves, in order to do\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GBfswT-XEeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}